run_name: llama_mia
seed: 42

rank: -1
local_rank: -1
pipe_parallel_size: 4
model_parallel_size: 1
world_size: 1
num_workers: 1
dp_num: 
pp_num:
mp_num:

per_gpu_train_batch_size: 1
per_gpu_eval_batch_size: 1
learning_rate: 5e-5
gradient_accumulation_steps: 32
weight_decay: 0
max_grad_norm: 1.0
num_train_epochs: 1
max_steps: -1
warmup_proportion: 0
warmup_steps: 100
prefetch_factor: 

model_init_path: 
output_dir: LLM_MIA/output/${run_name}
resume_from_checkpoint: 
data_name: mem_pretrain

max_train_steps: 100
eval_steps: 100
save_steps: 100
save_total_limit: 3
log_steps: 1
ds_log_steps: 50
resume_step: -1
model_max_length: 2048

run_test: false

gradient_checkpoint_interval: 0

ntk: false

hydra:  
  output_subdir: null
  run:  
    dir: .


# for hf trainer
training_arguments:
  run_name: ${run_name}
  local_rank: ${local_rank}
  seed: ${seed}

  output_dir: ${output_dir}
  logging_dir: ${output_dir}/logs

  per_device_train_batch_size: ${per_gpu_train_batch_size}
  per_device_eval_batch_size: ${per_gpu_eval_batch_size}

  gradient_accumulation_steps: ${gradient_accumulation_steps}
  learning_rate: ${learning_rate}
  bf16: true
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: ${max_grad_norm}
  num_train_epochs: ${num_train_epochs}
  lr_scheduler_type: cosine
  warmup_steps: ${warmup_steps}
  optim: adamw_torch
  report_to: tensorboard

  logging_steps: ${log_steps}
  save_steps: ${save_steps}
  save_total_limit: ${save_total_limit}
  save_safetensors: false

  resume_from_checkpoint: ${resume_from_checkpoint}
  gradient_checkpointing: true 
  



# model arguments is the same as Qwen1.5-0.5b
model_arguments:
  architectures: LlamaForCausalLM
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: silu
  hidden_size: 4096
  initializer_range: 0.02
  intermediate_size: 11008
  max_position_embeddings: 16384
  model_type: llama
  num_attention_heads: 32
  num_hidden_layers: 32
  num_key_value_heads: 32
  pretraining_tp: 1
  rms_norm_eps: 1e-05
  rope_scaling: 
  rope_theta: 1000000
  tie_word_embeddings: false
  torch_dtype: bfloat16
  use_cache: true
  vocab_size: 32016
  attn_implementation: 

tokenizer:
  tokenizer_path: cache/LLM_Weight/codellama-7b-base-hf

# Deepspeed config
deepspeed_config:
  # train_batch_size: ${per_gpu_train_batch_size}

  train_micro_batch_size_per_gpu: ${per_gpu_train_batch_size}
  gradient_accumulation_steps: ${gradient_accumulation_steps}
  gradient_clipping: 1.0
  steps_per_print: ${ds_log_steps}

  optimizer:
    type: AdamW
    params:
      lr: ${learning_rate}
      betas: [0.9, 0.99]
      eps: 1e-8
      weight_decay: ${weight_decay}

  scheduler:
    type: WarmupDecayLR
    params: 
      total_num_steps: 1000
      warmup_max_lr: ${learning_rate}
      warmup_num_steps: ${warmup_steps}
      warmup_type: linear

  # bf16:
  #   enabled: auto
  
  bf16:
    enabled: true

  # autotuning:
  #  enabled: true
  #  arg_mappings:
  #    train_micro_batch_size_per_gpu: "per_gpu_train_batch_size"
  #    gradient_accumulation_steps: "gradient_accumulation_steps"
  #    zero_optimization: "ds_cfg.zero_optimization"

  wall_clock_breakdown: true

  wandb:
    enabled: false 
    project: ${run_name}

  zero_optimization:
    stage: 1
    contiguous_gradients: true
    overlap_comm: true
    reduce_scatter: true
    reduce_bucket_size: 5e8
    allgather_partitions: true
    allgather_bucket_size: 5e8
    offload_optimizer:
      device: cpu
      pin_memory: true
